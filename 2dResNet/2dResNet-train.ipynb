{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_moments_dataset import Moments\n",
    "from global_params import *\n",
    "from Frame2dResNet50 import Frame2dResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training videos: 100000\n",
      "Number of validation videos: 10000\n"
     ]
    }
   ],
   "source": [
    "trainset = Moments(subset='training', use_frames=4)\n",
    "valset = Moments(subset='validation', use_frames=4)\n",
    "print(\"Number of training videos:\", len(trainset))\n",
    "print(\"Number of validation videos:\", len(valset))\n",
    "\n",
    "if (debug) :\n",
    "    video_info = trainset.__getitem__(3)\n",
    "    print(video_info[0].shape)\n",
    "    print(video_info[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=12)\n",
    "valset_loader = DataLoader(valset, batch_size=16, shuffle=True, num_workers=12)\n",
    "\n",
    "if (debug) :\n",
    "    for batch_idx, (data, target, _) in enumerate(trainset_loader):\n",
    "        print('batch', batch_idx)\n",
    "        print('data.shape=', data.shape)\n",
    "        print('target=', target)\n",
    "\n",
    "        if (batch_idx >= 1) :\n",
    "            break\n",
    "    print(\"\") \n",
    "    for batch_idx, (data, target, _) in enumerate(valset_loader):\n",
    "        print('batch', batch_idx)\n",
    "        print('data.shape=', data.shape)\n",
    "        print('target=', target)\n",
    "\n",
    "        if (batch_idx >= 1) :\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug :\n",
    "    model = Frame2dResNet50(use_pretrain=4).to(device)\n",
    "\n",
    "    model.train()\n",
    "    x = torch.zeros((64, 3, 16, 224, 224)).to(device)\n",
    "    scores = model(x)\n",
    "    print(scores.shape) ## Should give torch.Size([64, 200])\n",
    "    print(F.cross_entropy(scores, torch.zeros(64, dtype=torch.long, device=device)))  \n",
    "\n",
    "    model.eval()\n",
    "    x = torch.zeros((4, 3, 16, 224, 224)).to(device)\n",
    "    scores = model(x)\n",
    "    print(scores.shape) ## Should give torch.Size([64, 200])\n",
    "    print(F.cross_entropy(scores, torch.zeros(4, dtype=torch.long, device=device)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_state(checkpoint_path, model, optimizer):\n",
    "    state = {'state_dict': model.state_dict(),\n",
    "             'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_training_state(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_size = 2000, print_to = sys.stdout):\n",
    "    model.eval()\n",
    "    num_correct1 = 0\n",
    "    num_correct5 = 0\n",
    "    num_samples = 0\n",
    "    torch.manual_seed(123)\n",
    "    with torch.no_grad():\n",
    "        for data, target, _ in valset_loader:\n",
    "            data_g = data.to(device)\n",
    "            scores = model(data_g).data.cpu().numpy()\n",
    "            preds = scores.argsort(axis=1)[:,-5:]\n",
    "            target_np = target.numpy()\n",
    "            batch_sz = preds.shape[0]\n",
    "            num_correct1 += (preds[:,-1] == target_np).sum()\n",
    "            num_correct5 += sum([1 if target_np[i] in preds[i] else 0 for i in range(batch_sz)])\n",
    "            num_samples += batch_sz\n",
    "            if (debug) :\n",
    "                break\n",
    "            if (num_samples % 400 == 0) :\n",
    "                print('Tested [{}/{} ({:.2%})]'.format(\n",
    "                    num_samples, len(valset), num_samples / len(valset)), flush=True)\n",
    "    acc1 = 1.0*num_correct1/num_samples\n",
    "    acc5 = 1.0*num_correct5/num_samples\n",
    "    print('\\tValidation set accuracy: top-1 {}/{} ({:.2%}), top-5 {}/{} ({:.2%})'.format(\n",
    "        num_correct1, num_samples, acc1, num_correct5, num_samples, acc5), flush=True, file=print_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded from ../data/model_param_saved/2dResNet-p4lr4-3.pth\n",
      "\tValidation set accuracy: top-1 1806/10000 (18.06%), top-5 4071/10000 (40.71%)\n"
     ]
    }
   ],
   "source": [
    "def testLoad() :\n",
    "    pre_imgnet = 4\n",
    "    nllr = 4\n",
    "    pre_ours = 3\n",
    "    param_val = 'p%dlr%d'%(pre_imgnet, nllr)\n",
    "\n",
    "    model = Frame2dResNet50().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=10**(-nllr))\n",
    "    load_training_state(os.path.join(savedPath, '2dResNet-'+param_val+'-3.pth'), model, optimizer)\n",
    "    torch.manual_seed(123)\n",
    "    test(model)\n",
    "    \n",
    "testLoad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save(epoch, model, optimizer, param_val, print_to = sys.stdout, epoc_start = 0):\n",
    "    \"\"\"\n",
    "    @pre: the return of @a model is the score of each category, which we will use cross-entropy loss\n",
    "    \"\"\"\n",
    "    print(param_val)\n",
    "    model.train()  # set training mode\n",
    "    iteration = 0\n",
    "    for ep in range(epoc_start, epoc_start+epoch):\n",
    "        t0 = time.time()\n",
    "        for batch_idx, (data, target, _) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(data)\n",
    "            loss = F.cross_entropy(scores, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if iteration % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()), flush=True, file=print_to)\n",
    "            if batch_idx == 800 :\n",
    "                test(model, print_to=print_to)\n",
    "                model.train()\n",
    "            iteration += 1\n",
    "        save_training_state(os.path.join(savedPath, '2dResNet-'+param_val+'-%d.pth'%(ep+1)), model, optimizer)\n",
    "        test(model, print_to=print_to)\n",
    "        model.train()\n",
    "        t1 = time.time()\n",
    "        print(\"Epoch %d done, takes %fs\"%(ep+1, t1-t0), flush=True)\n",
    "        print(\"Epoch %d done, takes %fs\"%(ep+1, t1-t0), flush=True, file=print_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(123)\n",
    "# model = FrameResNet50(use_pretrain=3).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# train_save(5, model, optimizer)\n",
    "# # load_checkpoint(\"../data/results/2d_resnet-2.pth\", model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuneparam(pre_imgnet, nllr, use_pre_ours=0, epocs=3) :\n",
    "    torch.manual_seed(123)\n",
    "    param_val = 'p%dlr%d'%(pre_imgnet, nllr)\n",
    "    if (use_pre_ours == 0) :\n",
    "        model = Frame2dResNet50(use_pretrain=pre_imgnet).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=10**(-nllr))\n",
    "        torch.manual_seed(123)\n",
    "    else :\n",
    "        model = Frame2dResNet50().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=10**(-nllr))\n",
    "        load_training_state(os.path.join(savedPath, '2dResNet-'+param_val+'-%d.pth'%(use_pre_ours)), model, optimizer)\n",
    "        torch.manual_seed(123)\n",
    "    log_file = open('log/log-p%dlr%d.txt'%(pre_imgnet, nllr), 'a')\n",
    "    train_save(epocs, model, optimizer, param_val, print_to=log_file, epoc_start=use_pre_ours)\n",
    "    log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_options = [(4, 4), (4, 3), (3, 4), (3, 3)]\n",
    "# for pre, nllr in param_options :\n",
    "#     tuneparam(pre, nllr, pre_ours=0, epocs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p4lr5\n",
      "model saved to ../data/model_param_saved/2dResNet-p4lr5-1.pth\n",
      "Epoch 1 done, takes 2789.668236s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-70:\n",
      "Process Process-72:\n",
      "Process Process-68:\n",
      "Process Process-69:\n",
      "Process Process-62:\n",
      "Process Process-67:\n",
      "Process Process-65:\n",
      "Process Process-71:\n",
      "Process Process-66:\n",
      "Process Process-64:\n",
      "Process Process-63:\n",
      "Process Process-61:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 61, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 61, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 61, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 61, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 61, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 61, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 61, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fcbeddf1ba8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 347, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/shared/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 178, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 4048) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f3e1bbb1a13d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinetuneparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pre_ours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-82d00e2e88e3>\u001b[0m in \u001b[0;36mfinetuneparam\u001b[0;34m(pre_imgnet, nllr, use_pre_ours, epocs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlog_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log/log-p%dlr%d.txt'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_imgnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnllr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoc_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_pre_ours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mlog_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d0b6bb3dbe44>\u001b[0m in \u001b[0;36mtrain_save\u001b[0;34m(epoch, model, optimizer, param_val, print_to, epoc_start)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "finetuneparam(4, 5, use_pre_ours=0, epocs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
